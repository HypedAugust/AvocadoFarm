{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weighted sum of the inputs at the first node in the hidden layer is 0.5825\n",
      "0.6416424532015634\n"
     ]
    }
   ],
   "source": [
    "# 원리 이해 \n",
    "\n",
    "weights = [0.55,0.45] # initialize the weights\n",
    "biases = [0.15] # initialize the biases\n",
    "\n",
    "# set input \n",
    "x_1 = 0.5 # input 1\n",
    "x_2 = 0.35 # input 2\n",
    "\n",
    "# compute weighted sum of the inputs , 1st hidden layer \n",
    "z_11 = x_1 * weights[0] + x_2 * weights[1] + biases[0]\n",
    "print('The weighted sum of the inputs at the first node in the hidden layer is {}'.format(z_11))\n",
    "\n",
    "# use sigmoid activation function \n",
    "a_11 = 1.0 / (1.0 + np.exp(-z_11))\n",
    "\n",
    "\n",
    "print(a_11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기화 (인풋, 히든레이어 갯수, 히든 노드 갯수, 아웃풋 노드 갯수)\n",
    "def initialize_network(num_inputs, num_hidden_layers, num_nodes_hidden, num_nodes_output):\n",
    "    \n",
    "    num_nodes_previous = num_inputs # number of nodes in the previous layer # 직전 노드 갯수 = 시작을 인풋 갯수로 한다. \n",
    "\n",
    "    network = {} # 전체 네트워크(레이어와 노드 정보)를 딕셔너리 형태로 저장하기 위해 초기화합니다.\n",
    "    \n",
    "    # loop through each layer and randomly initialize the weights and biases associated with each layer # 모든 레이어를 초기화 하는데 이때 웨이트랑 바이아스 모두 초기화 한다. \n",
    "    for layer in range(num_hidden_layers + 1): # 히든 레이어를 하나하나 돈다. \n",
    "        \n",
    "        if layer == num_hidden_layers: # 만약에 히든레이어까지 다 돌고나면 \n",
    "            layer_name = 'output' # name last layer in the network output # 해당 레이어는 output로 한다. 마지막 레이어니까. \n",
    "            num_nodes = num_nodes_output # 그 마지막 레이어의 노드는 num_nodes_output으로 바꿔준다. \n",
    "        else:\n",
    "            layer_name = 'layer_{}'.format(layer + 1) # otherwise give the layer a number # 예시 layer = 0 이면, layer_1로 레이어 이름을 문자열화 해준다. \n",
    "            num_nodes = num_nodes_hidden[layer] \n",
    "        \n",
    "        # initialize weights and bias for each node # 각 노드는 “이전 레이어에서 오는 입력”마다 가중치(여러 개)를 가지며, 추가로 편향(bias)도 하나 가집니다.\n",
    "        network[layer_name] = {} # network[\"layer_1\"] = {}\n",
    "        for node in range(num_nodes): # 전체 노드에 대해서 \n",
    "            node_name = 'node_{}'.format(node+1) # 각 노드에 레이블 달아준다. \n",
    "            network[layer_name][node_name] = { # network가 2중 딕셔너리 구조가 된다. 예시 network[\"layer_1\"][\"node_1\"]\n",
    "                'weights': np.around(np.random.uniform(size=num_nodes_previous), decimals=2),\n",
    "                'bias': np.around(np.random.uniform(size=1), decimals=2),\n",
    "            }\n",
    "    \n",
    "        num_nodes_previous = num_nodes # 레이어 생성 후, 다음 레이어가 몇 개 가중치를 가져야 할지 알려주기 위해 현재 레이어 노드 수를 ‘이전 레이어 노드 수’로 갱신합니다.\n",
    "\n",
    "    return network # return the network\n",
    "\n",
    "\n",
    "# Define the small network\n",
    "small_network = initialize_network(num_inputs=5, num_hidden_layers=1, num_nodes_hidden=[3], num_nodes_output=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_sum(inputs, weights, bias): # weighted sum을 해줘야 다음 레이어로 넘어간다. \n",
    "    return np.sum(inputs * weights) + bias\n",
    "\n",
    "from random import seed\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(12)\n",
    "inputs = np.around(np.random.uniform(size=5), decimals=2)\n",
    "\n",
    "print('The inputs to the network are {}'.format(inputs))\n",
    "\n",
    "node_weights = small_network['layer_1']['node_1']['weights']\n",
    "node_bias = small_network['layer_1']['node_1']['bias']\n",
    "weighted_sum = compute_weighted_sum(inputs, node_weights, node_bias) \n",
    "print('The weighted sum at the first node in the hidden layer is {}'.format(np.around(weighted_sum[0], decimals=4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_activation(weighted_sum):\n",
    "    return 1.0 / (1.0 + np.exp(-1 * weighted_sum))\n",
    "\n",
    "node_output  = node_activation(compute_weighted_sum(inputs, node_weights, node_bias))\n",
    "print('The output of the first node in the hidden layer is {}'.format(np.around(node_output[0], decimals=4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation = (Initialization) + (Weighted Sum) + (Node Activation) for every node in every layer, sequentially.\n",
    "\n",
    "\n",
    "def forward_propagate(network, inputs): \n",
    "    \n",
    "    layer_inputs = list(inputs) # start with the input layer as the input to the first hidden layer\n",
    "    \n",
    "    for layer in network:\n",
    "        \n",
    "        layer_data = network[layer] # 예: network['layer_1'] → 'layer_1'이 layer이고, 레이어 정보가 layer_data에 저장됩니다.\n",
    "        \n",
    "        layer_outputs = [] # 한 레이어에는 여러 노드가 있으며, 각 노드의 출력값을 저장해야 하기 때문입니다.\n",
    "        for layer_node in layer_data: \n",
    "\n",
    "# layer_data = {\n",
    "#     'node_1': {'weights': [...], 'bias': ...},\n",
    "#     'node_2': {'weights': [...], 'bias': ...}\n",
    "# }\n",
    "        \n",
    "            node_data = layer_data[layer_node] # node_data = layer_data['node_1']\n",
    "        \n",
    "            # compute the weighted sum and the output of each node at the same time \n",
    "            node_output = node_activation(compute_weighted_sum(layer_inputs, node_data['weights'], node_data['bias']))\n",
    "            layer_outputs.append(np.around(node_output[0], decimals=4))\n",
    "            \n",
    "        if layer != 'output':\n",
    "            print('The outputs of the nodes in hidden layer number {} is {}'.format(layer.split('_')[1], layer_outputs)) \n",
    "    \n",
    "        layer_inputs = layer_outputs # set the output of this layer to be the input to next layer\n",
    "\n",
    "    network_predictions = layer_outputs\n",
    "    return network_predictions\n",
    "\n",
    "predictions = forward_propagate(small_network, inputs)\n",
    "print('The predicted value by the network for the given input is {}'.format(np.around(predictions[0], decimals=4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 코드 \n",
    "def initialize_network(num_inputs, num_hidden_layers, num_nodes_hidden, num_nodes_output):\n",
    "    \n",
    "    num_nodes_previous = num_inputs # number of nodes in the previous layer\n",
    "\n",
    "    network = {}\n",
    "    \n",
    "    # loop through each layer and randomly initialize the weights and biases associated with each layer\n",
    "    for layer in range(num_hidden_layers + 1):\n",
    "        \n",
    "        if layer == num_hidden_layers:\n",
    "            layer_name = 'output' # name last layer in the network output\n",
    "            num_nodes = num_nodes_output\n",
    "        else:\n",
    "            layer_name = 'layer_{}'.format(layer + 1) # otherwise give the layer a number\n",
    "            num_nodes = num_nodes_hidden[layer] \n",
    "        \n",
    "        # initialize weights and bias for each node\n",
    "        network[layer_name] = {}\n",
    "        for node in range(num_nodes):\n",
    "            node_name = 'node_{}'.format(node+1)\n",
    "            network[layer_name][node_name] = {\n",
    "                'weights': np.around(np.random.uniform(size=num_nodes_previous), decimals=2),\n",
    "                'bias': np.around(np.random.uniform(size=1), decimals=2),\n",
    "            }\n",
    "    \n",
    "        num_nodes_previous = num_nodes\n",
    "\n",
    "    return network # return the network\n",
    "\n",
    "def compute_weighted_sum(inputs, weights, bias):\n",
    "    return np.sum(inputs * weights) + bias\n",
    "\n",
    "from random import seed\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(12)\n",
    "inputs = np.around(np.random.uniform(size=5), decimals=2)\n",
    "\n",
    "print('The inputs to the network are {}'.format(inputs))\n",
    "\n",
    "node_weights = small_network['layer_1']['node_1']['weights']\n",
    "node_bias = small_network['layer_1']['node_1']['bias']\n",
    "weighted_sum = compute_weighted_sum(inputs, node_weights, node_bias) # 여기서 input은 뭐가 됨? \n",
    "print('The weighted sum at the first node in the hidden layer is {}'.format(np.around(weighted_sum[0], decimals=4)))\n",
    "\n",
    "def node_activation(weighted_sum):\n",
    "    return 1.0 / (1.0 + np.exp(-1 * weighted_sum))\n",
    "\n",
    "node_output  = node_activation(compute_weighted_sum(inputs, node_weights, node_bias))\n",
    "print('The output of the first node in the hidden layer is {}'.format(np.around(node_output[0], decimals=4)))\n",
    "\n",
    "\n",
    "\n",
    "# The way we are going to accomplish this is through the following procedure:\n",
    "\n",
    "# Start with the input layer as the input to the first hidden layer.\n",
    "# Compute the weighted sum at the nodes of the current layer.\n",
    "# Compute the output of the nodes of the current layer.\n",
    "# Set the output of the current layer to be the input to the next layer.\n",
    "# Move to the next layer in the network.\n",
    "# Repeat steps 2 - 4 until we compute the output of the output layer.\n",
    "\n",
    "def forward_propagate(network, inputs):\n",
    "    \n",
    "    layer_inputs = list(inputs) # start with the input layer as the input to the first hidden layer\n",
    "    \n",
    "    for layer in network:\n",
    "        \n",
    "        layer_data = network[layer]\n",
    "        \n",
    "        layer_outputs = [] \n",
    "        for layer_node in layer_data:\n",
    "        \n",
    "            node_data = layer_data[layer_node]\n",
    "        \n",
    "            # compute the weighted sum and the output of each node at the same time \n",
    "            node_output = node_activation(compute_weighted_sum(layer_inputs, node_data['weights'], node_data['bias']))\n",
    "            layer_outputs.append(np.around(node_output[0], decimals=4))\n",
    "            \n",
    "        if layer != 'output':\n",
    "            print('The outputs of the nodes in hidden layer number {} is {}'.format(layer.split('_')[1], layer_outputs))\n",
    "    \n",
    "        layer_inputs = layer_outputs # set the output of this layer to be the input to next layer\n",
    "\n",
    "    network_predictions = layer_outputs\n",
    "    return network_predictions\n",
    "\n",
    "predictions = forward_propagate(small_network, inputs)\n",
    "print('The predicted value by the network for the given input is {}'.format(np.around(predictions[0], decimals=4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
