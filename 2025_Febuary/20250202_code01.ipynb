{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "1) Logit : The raw score produced by a NN before applying the softmax function \n",
    "2) Embedding bag : A method that combines multiple word Embeddings into a single vector using averaging or summation. \n",
    "3) Bag of words : A method to represent text using the count of words without considering word order. \n",
    "4) Data loader : A tool in Pytorch that loads data in mini-batches for efficient training.\n",
    "5) Vocabulary Size : The total number of words in the model's vocabulary. \n",
    "6) Feedforward NN : A NN where data flows in one direction only. \n",
    "7) RNN : A NN designed to process sequential data while remembering past information \n",
    "8) FC layer : a layer where every neuron is connected to every neuron in the next layer. \n",
    "9) one hot encoding \n",
    "10) CBOW : A word2vec model that predicts a target word using surrounding context words. \n",
    "11) Hyper parameter \n",
    "12) Loss function \n",
    "13) Optimization \n",
    "14) Monte Carlo Sampling : A technique that estimates probability distributions by averaging over multiple random samples. \n",
    "15) Cross entropy loss : A loss Function used for classifiction problems that measures the diffence between predicted and actual probabilities. \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
