{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device for training\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "split = 'train'\n",
    "\n",
    "# Device for training\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "split = 'train'\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 3e-4\n",
    "batch_size = 64\n",
    "max_iters = 5000              # Maximum training iterations\n",
    "eval_interval = 200           # Evaluate model every 'eval_interval' iterations in the training loop\n",
    "eval_iters = 100              # When evaluating, approximate loss using 'eval_iters' batches\n",
    "\n",
    "# Architecture parameters\n",
    "max_vocab_size = 256          # Maximum vocabulary size\n",
    "vocab_size = max_vocab_size   # Real vocabulary size (e.g. BPE has a variable length, so it can be less than 'max_vocab_size')\n",
    "block_size = 16               # Context length for predictions\n",
    "n_embd = 32                   # Embedding size\n",
    "num_heads = 2                 # Number of head in multi-headed attention\n",
    "n_layer = 2                   # Number of Blocks\n",
    "ff_scale_factor = 4           # Note: The '4' magic number is from the paper: In equation 2 uses d_model=512, but d_ff=2048\n",
    "dropout = 0.0                 # Normalization using dropout# 10.788929 M parameters\n",
    "\n",
    "head_size = n_embd // num_heads\n",
    "assert (num_heads * head_size) == n_embd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split a sentence into tokens (words)\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    This function takes a string of text as input and returns a list of words (tokens).\n",
    "    It uses the split method, which by default splits on any whitespace, to tokenize the text.\n",
    "    \"\"\"\n",
    "    return text.split()  # Split the input text on whitespace and return the list of tokens\n",
    "\n",
    "# Function to translate a sentence from source to target language word by word\n",
    "def translate(sentence):\n",
    "    \"\"\"\n",
    "    This function translates a sentence by looking up each word's translation in a predefined dictionary.\n",
    "    It assumes that every word in the sentence is a key in the dictionary.\n",
    "    \"\"\"\n",
    "    out = ''  # Initialize the output string\n",
    "    for token in tokenize(sentence):  # Tokenize the sentence into words\n",
    "        # Append the translated word to the output string\n",
    "        # This line assumes the dictionary contains a translation for every word in the input\n",
    "        out += dictionary[token] + ' '\n",
    "    return out.strip()  # Return the translated sentence, stripping any extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the closest key in the dictionary to the given query word\n",
    "def find_closest_key(query):\n",
    "    \"\"\"\n",
    "    The function computes the Levenshtein distance between the query and each key in the dictionary.\n",
    "    The Levenshtein distance is a measure of the number of single-character edits required to change one word into the other.\n",
    "    \"\"\"\n",
    "    closest_key, min_dist = None, float('inf')  # Initialize the closest key and minimum distance to infinity\n",
    "    for key in dictionary.keys():\n",
    "        dist = distance(query, key)  # Calculate the Levenshtein distance to the current key\n",
    "        if dist < min_dist:  # If the current distance is less than the previously found minimum\n",
    "            min_dist, closest_key = dist, key  # Update the minimum distance and the closest key\n",
    "    return closest_key  # Return the closest key found\n",
    "\n",
    "# Function to translate a sentence from source to target language using the dictionary\n",
    "def translate(sentence):\n",
    "    \"\"\"\n",
    "    This function tokenizes the input sentence into words and finds the closest translation for each word.\n",
    "    It constructs the translated sentence by appending the translated words together.\n",
    "    \"\"\"\n",
    "    out = ''  # Initialize the output string\n",
    "    for query in tokenize(sentence):  # Tokenize the sentence into words\n",
    "        key = find_closest_key(query)  # Find the closest key in the dictionary for each word\n",
    "        out += dictionary[key] + ' '  # Append the translation of the closest key to the output string\n",
    "    return out.strip()  # Return the translated sentence, stripping any extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DL \n",
    "# Create and sort the input vocabulary from the dictionary's keys\n",
    "vocabulary_in = sorted(list(set(dictionary.keys())))\n",
    "# Display the size and the sorted vocabulary for the input language\n",
    "print(f\"Vocabulary input ({len(vocabulary_in)}): {vocabulary_in}\")\n",
    "\n",
    "# Create and sort the output vocabulary from the dictionary's values\n",
    "vocabulary_out = sorted(list(set(dictionary.values())))\n",
    "# Display the size and the sorted vocabulary for the output language\n",
    "print(f\"Vocabulary output ({len(vocabulary_out)}): {vocabulary_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a list of vocabulary words into one-hot encoded vectors\n",
    "def encode_one_hot(vocabulary):\n",
    "    vocabulary_size = len(vocabulary)  # Get the size of the vocabulary\n",
    "    one_hot = dict()  # Initialize a dictionary to hold our one-hot encodings\n",
    "    LEN = len(vocabulary)  # The length of each one-hot encoded vector will be equal to the vocabulary size\n",
    "    \n",
    "    # Iterate over the vocabulary to create a one-hot encoded vector for each word\n",
    "    for i, key in enumerate(vocabulary):\n",
    "        one_hot_vector = torch.zeros(LEN)  # Start with a vector of zeros\n",
    "        one_hot_vector[i] = 1  # Set the i-th position to 1 for the current word\n",
    "        one_hot[key] = one_hot_vector  # Map the word to its one-hot encoded vector\n",
    "        print(f\"{key}\\t: {one_hot[key]}\")  # Print each word and its encoded vector\n",
    "    \n",
    "    return one_hot  # Return the dictionary of words and their one-hot encoded vectors\n",
    "\n",
    "# Apply the one-hot encoding function to the input vocabulary and store the result\n",
    "one_hot_in = encode_one_hot(vocabulary_in)\n",
    "\n",
    "# Iterate over the one-hot encoded input vocabulary and print each vector\n",
    "# This visualizes the one-hot representation for each word in the input vocabulary\n",
    "for k, v in one_hot_in.items():\n",
    "    print(f\"E_{{ {k} }} = \" , v)\n",
    "\n",
    "# Apply the one-hot encoding function to the output vocabulary and store the result\n",
    "# This time we're encoding the target language vocabulary\n",
    "one_hot_out = encode_one_hot(vocabulary_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking the one-hot encoded vectors for input vocabulary to form a tensor\n",
    "K = torch.stack([one_hot_in[k] for k in dictionary.keys()])\n",
    "# K now represents a matrix of one-hot vectors for the input vocabulary\n",
    "\n",
    "# Display the tensor for verification\n",
    "print(K)\n",
    "\n",
    "# Similarly, stack the one-hot encoded vectors for output vocabulary to form a tensor\n",
    "V = torch.stack([one_hot_out[k] for k in dictionary.values()])\n",
    "# V represents the corresponding matrix of one-hot vectors for the output vocabulary\n",
    "\n",
    "# Display the tensor for verification\n",
    "print(V)\n",
    "\n",
    "# Demonstrating how to look up a translation for a given word using matrix operations\n",
    "# Here, we take the one-hot representation of 'sous' from the input vocabulary\n",
    "q = one_hot_in['sous']\n",
    "# Display the query token vector\n",
    "print(\"Query token :\", q)\n",
    "\n",
    "# Select the corresponding key vector in K (input dictionary matrix) using matrix multiplication\n",
    "# This operation gives us the index where 'sous' would be '1' in the one-hot encoded input matrix\n",
    "print(\"Select key (K) :\", q @ K.T)\n",
    "\n",
    "# Use the index found from the key selection to find the corresponding value vector in V (output dictionary matrix)\n",
    "# This operation selects the row from V that is the translation of 'sous' in the output vocabulary\n",
    "print(\"Select value (V):\", q @ K.T @ V)\n",
    "\n",
    "# The final output demonstrates how 'sous' can be translated using the neural network approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_one_hot(one_hot, vector):\n",
    "    \"\"\" \n",
    "    Decode a one-hot encoded vector to find the best matching token in the vocabulary.\n",
    "    \"\"\"\n",
    "    best_key, best_cosine_sim = None, 0\n",
    "    for k, v in one_hot.items():  # Iterate over the one-hot encoded vocabulary\n",
    "        cosine_sim = torch.dot(vector, v)  # Calculate dot product (cosine similarity)\n",
    "        if cosine_sim > best_cosine_sim:  # If this is the best similarity we've found\n",
    "            best_cosine_sim, best_key = cosine_sim, k  # Update the best similarity and token\n",
    "    return best_key  # Return the token corresponding to the one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    \"\"\" \n",
    "    Translate a sentence using matrix multiplication, treating the dictionaries as matrices.\n",
    "    \"\"\"\n",
    "    sentence_out = ''  # Initialize the output sentence\n",
    "    for token_in in tokenize(sentence):  # Tokenize the input sentence\n",
    "        q = one_hot_in[token_in]  # Find the one-hot vector for the token\n",
    "        out = q @ K.T @ V  # Multiply with the input and output matrices to find the translation\n",
    "        token_out = decode_one_hot(one_hot_out, out)  # Decode the output one-hot vector to a token\n",
    "        sentence_out += token_out + ' '  # Append the translated token to the output sentence\n",
    "    return sentence_out.strip()  # Return the translated sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation with attention mechanism \n",
    "def translate(sentence):\n",
    "    \"\"\"\n",
    "    Translate a sentence using the attention mechanism represented by the K and V matrices.\n",
    "    The softmax function is used to calculate a weighted sum of the V vectors, focusing on the most relevant vector for translation.\n",
    "    \"\"\"\n",
    "    sentence_out = ''  # Initialize the output sentence\n",
    "    for token_in in tokenize(sentence):  # Tokenize the input sentence\n",
    "        q = one_hot_in[token_in]  # Get the one-hot vector for the current token\n",
    "        # Apply softmax to the scaled dot product of q and K.T, then multiply by V\n",
    "        # This selects the most relevant translation vector from V\n",
    "        out = torch.softmax(q @ K.T, dim=0) @ V\n",
    "        token_out = decode_one_hot(one_hot_out, out)  # Decode the output vector to a token\n",
    "        sentence_out += token_out + ' '  # Append the translated token to the output sentence\n",
    "    return sentence_out.strip()  # Return the translated sentence\n",
    "\n",
    "# Test the translate function\n",
    "translate(\"le chat est sous la table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translation process by handling all queries in parallel\n",
    "# Creating the 'Q' matrix\n",
    "\n",
    "# The sentence we want to translate\n",
    "sentence = \"le chat est sous la table\"\n",
    "\n",
    "# Stack all the one-hot encoded vectors for the tokens in the sentence to form the Q matrix\n",
    "Q = torch.stack([one_hot_in[token] for token in tokenize(sentence)])\n",
    "\n",
    "# Display the Q matrix\n",
    "print(Q)\n",
    "\n",
    "def translate(sentence):\n",
    "    \"\"\"\n",
    "    Translate a sentence using matrix multiplication in parallel.\n",
    "    This function replaces the iterative approach with a single matrix multiplication step,\n",
    "    applying the attention mechanism across all tokens at once.\n",
    "    \"\"\"\n",
    "    # Tokenize the sentence and stack the one-hot vectors to form the Q matrix\n",
    "    Q = torch.stack([one_hot_in[token] for token in tokenize(sentence)])\n",
    "    \n",
    "    # Apply softmax to the dot product of Q and K.T and multiply by V\n",
    "    # This will give us the output vectors for all tokens in parallel\n",
    "    out = torch.softmax(Q @ K.T, 0) @ V\n",
    "    \n",
    "    # Decode each one-hot vector in the output to the corresponding token\n",
    "    # And join the tokens to form the translated sentence\n",
    "    return ' '.join([decode_one_hot(one_hot_out, o) for o in out])\n",
    "    \n",
    "# Test the function to ensure it produces the correct translation\n",
    "translate(\"le chat est sous la table\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
